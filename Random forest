[1] Load & Explore Data
       |
       v
[2] Visualize Data (Scatter Plot)
       |
       v
[3] One-Hot Encode Categorical Variables
       |
       v
[4] Separate Target Variable (y)
       |
       v
[5] Drop Irrelevant Columns
       |
       v
[6] Convert to NumPy Arrays (X, y)
       |
       v
[7] Train/Test Split (75/25)
       |
       v
[8] Train Random Forest Model
       |
       v
[9] Predict on Test Data
       |
       v
[10] Calculate MAE and Accuracy



[1] Load & Explore Data
Begin by loading the dataset using pandas.read_csv() and explore its structure using functions like .head(), .info(), and .describe().
This helps identify column names, data types, summary statistics, and any missing values.
Early exploration is important for understanding the data distribution and preparing for preprocessing.
It ensures you don't proceed with inconsistent, noisy, or incomplete data.

[2] Visualize Data (Scatter Plot)
Visualization is a crucial step to understand the relationship between variables.
Using matplotlib.pyplot.scatter(), we plot features like temperature vs. actual values to identify trends, clusters, or outliers.
Colors (c=y) may be used to differentiate classes or outputs.
This step can guide feature selection and model choice.
Good visualizations make patterns in the data easier to interpret.

[3] One-Hot Encode Categorical Variables
Many machine learning algorithms require numerical input, so categorical variables must be encoded.
Using pd.get_dummies(), each category is converted into a new column with binary values (0 or 1).
This avoids assigning arbitrary numeric values to categories, which could mislead the model.
For example, weather types like "Sunny", "Cloudy", and "Rainy" become separate columns.
Proper encoding ensures that the model can learn meaningful patterns from categorical data.

[4] Separate Target Variable (y)
The column to be predicted (e.g., actual temperature) is extracted and stored separately as the target variable y.
This helps in clearly separating what the model needs to learn from what it uses as input (X).
Target separation is essential before training any model.
In regression problems, this target is usually a continuous variable.
It is also useful for error analysis and evaluation after prediction.

[5] Drop Irrelevant Columns
Columns that don’t contribute meaningfully to prediction or cause information leakage (e.g., future forecasts or IDs) are dropped using df.drop().
Removing such columns reduces noise and complexity in the model.
It ensures that only the most relevant features are used for training.
This step is part of feature selection and improves model performance.
Too many irrelevant features can lead to overfitting or poor accuracy.

[6] Convert to NumPy Arrays (X, y)
After finalizing the feature set and target variable, they are converted into NumPy arrays using np.array().
NumPy arrays are faster and more efficient in numerical computations, especially for machine learning models.
Libraries like scikit-learn expect inputs in array format.
This conversion also enables mathematical operations like distance computation or matrix multiplication.
It is a standard preprocessing step before modeling.

[7] Train/Test Split (75/25)
Use train_test_split() to divide the dataset into training and test sets (typically 75% for training, 25% for testing).
The model learns from the training data and is evaluated on the test set.
This helps assess how well the model generalizes to unseen data.
A fixed random_state ensures reproducible splits.
Balanced splits prevent overfitting and provide reliable evaluation.

[8] Train Random Forest Model
A RandomForestRegressor is initialized with parameters like n_estimators (number of trees) and random_state.
The model is trained on X_train and y_train using .fit().
Random Forest is an ensemble learning method that builds many decision trees and averages their outputs.
It handles both classification and regression tasks effectively.
It reduces overfitting and improves accuracy by using randomness in tree building.

[9] Predict on Test Data
Once trained, the model predicts target values for X_test using .predict().
These predicted values are compared to actual test labels to measure how accurate the model is.
Prediction is the core goal of building the model—to make future estimates based on learned data.
Well-trained models produce predictions that are close to real values.
The quality of prediction depends on the model and data preparation.

[10] Calculate MAE and Accuracy
To evaluate model performance, compute Mean Absolute Error (MAE), which gives the average error in predictions.
Also calculate MAPE (Mean Absolute Percentage Error) to estimate how far predictions are from actuals in percentage terms.
From MAPE, calculate accuracy as 100 - average MAPE.
These metrics give insights into how reliable the model is for practical use.
Lower MAE and higher accuracy indicate a better-performing model.





import pd
np
head
info
shape
describe
import plt

plt.scatter(list(range(1,349)),df['actual'])
temp_2	
temp_1
forecast_noaa
friend

df = pd.get_dummies(df)

df.head()

y = np.array(df['actual'])
y.shape
df =df.drop(['actual'],axis=1)
df=df.drop('forecast_noaa',axis=1)
df=df.drop('forecast_acc',axis=1)
df=df.drop('forecast_under',axis=1)
df.head()
list(df.columns)

X =np.array(df)
X.shape
from sklearn.model_selection import train_test_split

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=5)
Xtrain.shape
Xtest.shape
ytrain.shape
ytest.shape

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor (n_estimators= 1000, random_state =5)

rf.fit(Xtrain ,ytrain)
pred =rf.predict(Xtest)
Xtest
pred
ytest
pred
ytest
errors =abs(pred - ytest)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')
import sklearn.metrics as met
met.median_absolute_error(pred,ytest)
mape = 100 * errors / ytest # Use 'erroes' instead of 'error'
accuracy = 100 - np.mean(mape)
print(accuracy)

Random forest 

explain each steps what input output and give flowsteps
